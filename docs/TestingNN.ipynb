{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimEventViewer as SMV\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib tk\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "from tqdm import trange\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# bins_dir is the numpy file containing the bin edges used to fit to\n",
    "bins_dir='/home/nr1315/Documents/Project/MachineLearning/test_bins.npy'\n",
    "\n",
    "# coeffs_file is the h5 file containing the effective dose coefficients\n",
    "coeffs_file='/home/nr1315/Documents/Project/effective_dose_coeffs.h5'\n",
    "\n",
    "testbins=np.load(bins_dir)\n",
    "coeffs=pd.read_hdf(coeffs_file)\n",
    "seed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffles data order\n",
    "def shuffle_inds(seed,batch):\n",
    "    n_val=int(0.2*batch)\n",
    "    torch.manual_seed(0)\n",
    "    shuffled_indices=torch.randperm(batch)\n",
    "    return shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a specific simulated dataset for training\n",
    "def SimTrainingInputs(file,avg,batch,shuffled_inds,seed):\n",
    "    a=SMV.NewSimPlotter()\n",
    "    path,name=os.path.split(file)\n",
    "    a.add_data(file,name[:-3])\n",
    "    df=a.data[name[:-3]]\n",
    "    cubes=df.query('CapCube>0')['CapCube'].values\n",
    "    z=(cubes/1e4).astype(int)\n",
    "    x=((cubes-z*1e4)/100).astype(int)\n",
    "    y=(cubes-z*1e4-x*100).astype(int)\n",
    "    x-=1\n",
    "    y-=1\n",
    "    z-=1\n",
    "    cubs=pd.DataFrame(np.array([x,y,z]).T,columns=['X','Y','Z'])\n",
    "    ze=np.zeros([4,4,4])\n",
    "    coords=np.argwhere(ze==0)\n",
    "    counts=[]\n",
    "    for coord in coords:\n",
    "        counts.append(np.count_nonzero((cubs.values==coord).all(axis=1)))\n",
    "    count=sum(counts)\n",
    "    ncounts=np.array(counts)/sum(counts)*avg\n",
    "    np.random.seed(seed)\n",
    "    train_x=np.random.poisson(np.tile(ncounts,(batch,1)))\n",
    "    train_x=64000*train_x/train_x.sum(axis=1)[:,np.newaxis]\n",
    "    train_x=torch.from_numpy(train_x).to(torch.float)\n",
    "    \n",
    "    n_val=int(0.2*batch)\n",
    "    if n_val>=1:\n",
    "        val_x=train_x[shuffled_inds[-n_val:]]\n",
    "        train_x=train_x[shuffled_inds[:-n_val]]\n",
    "    else:\n",
    "        val_x=None\n",
    "    return train_x,val_x,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a specific real dataset for training\n",
    "def RealTrainingInputs(file,avg,batch,shuffled_inds,seed):\n",
    "    file=pd.read_hdf(file)\n",
    "    file=file[file['Combine']]\n",
    "    cubes=file.loc[:,['CubeX','CubeY','CubeZ']]\n",
    "    ze=np.zeros([4,4,4])\n",
    "    coords=np.argwhere(ze==0)\n",
    "    counts=[]\n",
    "    for coord in coords:\n",
    "        counts.append(np.count_nonzero((cubes.values==coord).all(axis=1)))\n",
    "    counts=np.array(counts)/2\n",
    "    count=counts.sum()\n",
    "    ncounts=counts/counts.sum()*avg\n",
    "    np.random.seed(seed)\n",
    "    train_x=np.random.poisson(np.tile(ncounts,(batch,1)))\n",
    "    train_x=avg*train_x/train_x.sum(axis=1)[:,np.newaxis]\n",
    "    train_x=torch.from_numpy(train_x).to(torch.float)\n",
    "    \n",
    "    \n",
    "    n_val=int(0.2*batch)\n",
    "    if n_val>=1:\n",
    "        val_x=train_x[shuffled_inds[-n_val:]]\n",
    "        train_x=train_x[shuffled_inds[:-n_val]]\n",
    "    else:\n",
    "        val_x=None\n",
    "    return train_x,val_x,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads an expected energy bins file\n",
    "def LoadTrainingTargets(file,avg,batch,shuffled_inds):\n",
    "    train_y=np.nan_to_num(np.load(file))\n",
    "    train_y=avg*train_y/train_y.sum()\n",
    "    train_y=train_y[:,np.newaxis].repeat(batch,axis=1).T\n",
    "    train_y=torch.from_numpy(train_y).to(torch.float)\n",
    "    \n",
    "    n_val=int(0.2*batch)\n",
    "    if n_val>=1:\n",
    "        val_y=train_y[shuffled_inds[-n_val:]]\n",
    "        train_y=train_y[shuffled_inds[:-n_val]]\n",
    "    else:\n",
    "        val_y=None\n",
    "    return train_y,val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares training data from a list of data files, associated labels, and whether each is a \n",
    "# simulated or real dataset.\n",
    "def prep_training_inputs(avg, batch, shuffled_inds,files,labels,real,seed=0):\n",
    "    train_data={}\n",
    "    val_data={}\n",
    "    counts=[]\n",
    "    for i in range(len(files)):\n",
    "        if real[i]:\n",
    "            train,val,count=RealTrainingInputs(files[i],avg,batch,shuffled_inds,seed)\n",
    "        else:\n",
    "            train,val,count=SimTrainingInputs(files[i],avg,batch,shuffled_inds,seed)\n",
    "        train_data[labels[i]]=train\n",
    "        val_data[labels[i]]=val\n",
    "        counts.append(count)\n",
    "    train=torch.cat(tuple(train_data.values()))\n",
    "    try:\n",
    "        val=torch.cat(tuple(val_data.values()))\n",
    "    except TypeError:\n",
    "        val=None\n",
    "    return train,val,counts\n",
    "\n",
    "# Prepares training targets from a list of datafiles\n",
    "def prep_training_targets(avg,batch,shuffled_inds,files,labels):\n",
    "    train_data={}\n",
    "    val_data={}\n",
    "    for i in range(len(files)):\n",
    "        train_data[labels[i]],val_data[labels[i]]=LoadTrainingTargets(files[i],avg,batch,shuffled_inds)\n",
    "    train=torch.cat(tuple(train_data.values()))\n",
    "    try:\n",
    "        val=torch.cat(tuple(val_data.values()))\n",
    "    except TypeError:\n",
    "        val=None\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates effective dose from a bin distribution\n",
    "def calc_dose(binvals,centres,coeffs):\n",
    "    E=coeffs['Energy / MeV'].values\n",
    "    AP=coeffs['AP'].values\n",
    "    eff_dose=interp1d(E,AP)\n",
    "    dose=((binvals/400)*eff_dose(centres)).sum(axis=1)\n",
    "    return dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xtrain=['/home/nr1315/Documents/Project/NPL_2017-06-13/data/nemenix2_npl_2017-06-13_cf-4774_1m5_0deg_sc-stand_1V0_v12.h5',\n",
    "        '/home/nr1315/Documents/Project/NPL_2017-06-13/data/nemenix2_npl_2017-06-14_ambe-30-1679_1m5_+0deg_sc-stand_1V0_v67.h5',\n",
    "        '/home/nr1315/Documents/Project/NPL_2017-06-13/data/nemenix2_npl_2017-06-14_amli_1m5_+0deg_sc-stand_1V0_v55.h5',\n",
    "        '/home/nr1315/Documents/Project/Simulations/Data/Low_scatter_new/4x4normal/thermal.h5'\n",
    "       ]\n",
    "labels=['Cf',\n",
    "        'AmBe',\n",
    "        'AmLi',\n",
    "        'Thermal'\n",
    "       ]\n",
    "real=[True,\n",
    "      True,\n",
    "      True,\n",
    "      False\n",
    "     ]\n",
    "\n",
    "ytrain=['/home/nr1315/Documents/Project/MachineLearning/CfBinCounts.npy',\n",
    "        '/home/nr1315/Documents/Project/MachineLearning/AmBeBinCounts.npy',\n",
    "        '/home/nr1315/Documents/Project/MachineLearning/AmLiBinCounts.npy',\n",
    "        '/home/nr1315/Documents/Project/MachineLearning/ThermBinCounts.npy'\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=1250\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "shuf=shuffle_inds(0,batch)\n",
    "X_train,X_val,X_counts=prep_training_inputs(64000,batch,shuf,xtrain,labels,real,seed=seed)\n",
    "Y_train,Y_val=prep_training_targets(64000,batch,shuf,ytrain,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=RealTrainingInputs('/home/nr1315/Documents/Project/NPL_2017-06-13/data/nemenix2_npl_2017-06-13_cf-4774_1m5_0deg_sc-stand_1V0_v12.h5',64000,1000,np.arange(0,1000,1),0)\n",
    "x_test=torch.cat([x_test[0],x_test[1]])\n",
    "y_test=LoadTrainingTargets('/home/nr1315/Documents/Project/MachineLearning/CfBinCounts.npy',64000,1000,np.arange(0,1000,1))\n",
    "y_test=torch.cat([y_test[0],y_test[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(X, y, batchsize, shuffle=False):\n",
    "    indices = np.arange(len(X))\n",
    "    if shuffle: \n",
    "        indices = np.random.permutation(indices)\n",
    "    for start in range(0, len(indices), batchsize):\n",
    "        ix = indices[start: start + batchsize]\n",
    "        yield X[ix], y[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs,model,optimizer,loss_fn,train_x,val_x,train_y,val_y,batchsize):\n",
    "    train_loss_per_epoch=[]\n",
    "    val_loss_per_epoch=[]\n",
    "    weight_hist=[]\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        model.train(True)\n",
    "        train_loss=0\n",
    "        for batch_x,batch_y in iterate_minibatches(train_x,train_y,batchsize,shuffle=True):\n",
    "            batch_pred_y=model(batch_x)\n",
    "            batch_train_loss=loss_fn(batch_pred_y,batch_y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            batch_train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss+=batch_train_loss.data.numpy()\n",
    "        train_loss_per_epoch.append(train_loss)\n",
    "        \n",
    "\n",
    "        \n",
    "        if val_x is not None:\n",
    "            val_loss=loss_fn(model(val_x),val_y)\n",
    "            val_loss_per_epoch.append(val_loss)\n",
    "\n",
    "        if epoch==1:\n",
    "            if val_x is not None:\n",
    "                print(f\"Epoch {epoch}, Training loss {train_loss:.4f},\"\n",
    "                      f\" Validation loss {val_loss.item():.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch}, Training loss {train_loss:.4f}\")\n",
    "        weight_hist.append(list(model.parameters()))\n",
    "            \n",
    "    return train_loss_per_epoch,val_loss_per_epoch,weight_hist\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_X_train=X_train/X_train.sum(axis=1)[:,np.newaxis]\n",
    "n_X_val=X_val/X_val.sum(axis=1)[:,np.newaxis]\n",
    "n_Y_train=Y_train/Y_train.sum(axis=1)[:,np.newaxis]\n",
    "n_Y_val=Y_val/Y_val.sum(axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 388717416.5000, Validation loss 2317353.0000\n",
      "\n",
      "Final training loss:  79519.21240234375, final validation loss:  tensor(5084.6782, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#model=nn.Sequential(OrderedDict([\n",
    "#        ('hidden_linear1',nn.Linear(64,128)),\n",
    "#        ('hidden_activation1',nn.Sigmoid()),\n",
    "#        ('hidden_linear2',nn.Linear(128,64)),\n",
    "#        ('hidden_activation2',nn.Sigmoid()),\n",
    "#        ('hidden_linear3',nn.Linear(64,32)),\n",
    "#        ('hidden_activation3',nn.Sigmoid()),\n",
    "#        ('output_layer',nn.Linear(32,24))]))\n",
    "\n",
    "        \n",
    "model=nn.Sequential(nn.Linear(64,128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128,64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64,32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(32,24),\n",
    "                    )\n",
    "    \n",
    "optimizer=optim.Rprop(model.parameters(),lr=1e-2)\n",
    "n_epochs=100\n",
    "\n",
    "def weights(m):\n",
    "    classname=m.__class__.__name__\n",
    "    \n",
    "    if classname=='Linear':\n",
    "        n=m.in_features\n",
    "        y=1.0/np.sqrt(n)\n",
    "        m.weight.data.normal_(0.0,y)\n",
    "        m.bias.data.fill_(4000)\n",
    "        \n",
    "model.apply(weights)\n",
    "\n",
    "tloss,vloss,weights=training_loop(n_epochs=n_epochs,\n",
    "                                  model=model,\n",
    "                                  optimizer=optimizer,\n",
    "                                  loss_fn=nn.MSELoss(),\n",
    "                                  train_x=X_train,\n",
    "                                  val_x=X_val,\n",
    "                                  train_y=Y_train,\n",
    "                                  val_y=Y_val,\n",
    "                                  batchsize=256)\n",
    "\n",
    "\n",
    "print()\n",
    "if len(vloss)>0:\n",
    "    print('Final training loss:  '+str(tloss[-1])+', final validation loss:  '+str(vloss[-1]))\n",
    "else:\n",
    "    print('Final training loss:  '+str(tloss[-1]))\n",
    "\n",
    "    \n",
    "if len(vloss)>0:\n",
    "    fig,ax=plt.subplots(2,1,figsize=(10,20))\n",
    "    ax[0].plot(np.arange(0,n_epochs,1),tloss,c='b',label='Training loss')\n",
    "    ax[0].set_title('Training loss',fontsize=24)\n",
    "    ax[1].plot(np.arange(0,n_epochs,1),vloss,c='r',label='Validation loss')\n",
    "    ax[1].set_title('Validation loss',fontsize=24)\n",
    "else:\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111)\n",
    "    ax.plot(np.arange(0,n_epochs,1),tloss,c='b',label='Training loss')\n",
    "    ax.set_title('Training loss',fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "barcentres=testbins[:-1]+np.diff(testbins)/2\n",
    "fig,ax=plt.subplots(4,2,figsize=(20,20))\n",
    "fig.set_tight_layout(True)\n",
    "title=['Cf','AmBe','AmLi','Thermal']\n",
    "for i in range(4):\n",
    "    ax[i,0].bar(barcentres,Y_val[250*i],edgecolor='xkcd:deep blue',width=np.diff(testbins))\n",
    "    ax[i,0].set_title(title[i]+' training data',fontsize=22)\n",
    "    ax[i,0].set_xscale('log')\n",
    "    ax[i,1].bar(barcentres,model(X_val[250*i]).detach().numpy(),edgecolor='xkcd:deep blue',width=np.diff(testbins))\n",
    "    ax[i,1].set_title(title[i]+' predicted data',fontsize=22)\n",
    "    ax[i,1].set_xscale('log')\n",
    "    ax[i,0].set_xlabel('Energy / MeV',fontsize=18)\n",
    "    ax[i,1].set_xlabel('Energy / MeV',fontsize=18)\n",
    "    ax[i,0].set_xlabel('Frequency',fontsize=18)\n",
    "    ax[i,1].set_xlabel('Frequency',fontsize=18)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=nn.MSELoss()\n",
    "l=loss(y_test,model(x_test)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,2,figsize=(20,10))\n",
    "ax[0].bar(barcentres,y_test[0],edgecolor='xkcd:deep blue',width=np.diff(testbins))\n",
    "ax[0].set_title('Test data')\n",
    "ax[1].bar(barcentres,model(x_test[0]).detach().numpy(),edgecolor='xkcd:deep blue',width=np.diff(testbins))\n",
    "ax[1].set_title('Predicted data')\n",
    "ax[0].set_xscale('log')\n",
    "ax[1].set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now load in an IPNDV dataset, to compare to Cf & the neural net predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataProcess as DP\n",
    "a=DP.Plotter()\n",
    "folder='/home/nr1315/Documents/Project/IPNDVdata/data_BR1/10-09-2019/'\n",
    "a.add_data(folder,'2019_09_10_bkg_0deg_0.95d_0.91h_afternoon.root','background')\n",
    "a.add_data(folder,'2019_09_10_pin96_Bare_0deg_0.root','96_bare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg=a.data['background']\n",
    "bkg=bkg[bkg['Combine']]\n",
    "bkgcubes=bkg.loc[:,['CubeX','CubeY','CubeZ']]\n",
    "\n",
    "data=a.data['96_bare']\n",
    "data=data[data['Combine']]\n",
    "cubes=data.loc[:,['CubeX','CubeY','CubeZ']]\n",
    "\n",
    "ze=np.zeros([4,4,4])\n",
    "coords=np.argwhere(ze==0)\n",
    "bkgcounts=[]\n",
    "counts=[]\n",
    "for coord in coords:\n",
    "    counts.append(np.count_nonzero((cubes.values==coord).all(axis=1)))\n",
    "    bkgcounts.append(np.count_nonzero((bkgcubes.values==coord).all(axis=1)))\n",
    "bkgcounts=np.array(bkgcounts)/2\n",
    "counts=np.array(counts)/2\n",
    "\n",
    "norm=(a.data['background']['time'].max()-a.data['background']['time'].min())/(a.data['96_bare']['time'].max()-a.data['96_bare']['time'].min())\n",
    "if norm>1:\n",
    "    counts=counts - bkgcounts/norm\n",
    "else:\n",
    "    counts=norm*counts - bkgcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=np.zeros([4,4,4])\n",
    "tt[coords[:,0],coords[:,1],coords[:,2]]=counts\n",
    "tt=np.rot90(tt,2,axes=(0,1))\n",
    "tcounts=tt.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Cf training inputs/target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,2,figsize=(20,10))\n",
    "\n",
    "ax[0].bar(np.arange(0,64,1),X_val[0],edgecolor='xkcd:deep blue',width=1)\n",
    "ax[0].set_xlabel('Cube ID',fontsize=18)\n",
    "ax[0].set_ylabel('Frequency',fontsize=18)\n",
    "ax[0].set_title('Cf training input',fontsize=22)\n",
    "\n",
    "ax[1].bar(barcentres,Y_val[0],edgecolor='xkcd:deep blue',width=np.diff(testbins))\n",
    "ax[1].set_xlabel('Energy / MeV',fontsize=18)\n",
    "ax[1].set_ylabel('Frequency',fontsize=18)\n",
    "ax[1].set_title('Cf training target',fontsize=22)\n",
    "ax[1].set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the IPNDV dataset prediction vs the Cf target used (i.e. comparing network prediction of IPNDV dataset with a known Cf fluence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig,ax=plt.subplots(1,2,figsize=(20,10))\n",
    "\n",
    "ax[1].bar(barcentres,model(torch.from_numpy(counts).to(torch.float)).detach().numpy(),edgecolor='xkcd:deep blue',width=np.diff(testbins))\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('Energy / MeV',fontsize=18)\n",
    "ax[1].set_ylabel('Frequency',fontsize=18)\n",
    "ax[1].set_title('Fluence prediction, 96 bare (IPNDV)',fontsize=22)\n",
    "\n",
    "ax[0].bar(barcentres,Y_val[0],edgecolor='xkcd:deep blue',width=np.diff(testbins))\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_xlabel('Energy / MeV',fontsize=18)\n",
    "ax[0].set_title('Cf fluence target',fontsize=22)\n",
    "ax[0].set_ylabel('Frequency',fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=np.rot90(tt,2,axes=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting 3D cube view of background subtracted IPNDV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ndap\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "sc=(50.888-50)/50\n",
    "p=ndap.NDArrayPlotter(tt)\n",
    "p.set_alpha(0.05)\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111,projection='3d')\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "cmap=cm.viridis\n",
    "norm=colors.Normalize(vmin=0,vmax=np.max(tt))\n",
    "p.colors=cmap(norm(tt))\n",
    "alph=norm(tt)*0.95\n",
    "p.alphas=alph\n",
    "sm=plt.cm.ScalarMappable(cmap=cmap,norm=norm)\n",
    "sm.set_array(tt)\n",
    "\n",
    "p.render(azim=-56,elev=25,ax=ax,text=None,labels=True,space=0.5)\n",
    "ax.quiver(-0.4,-0.4,-0.4,1,0,0,length=5,arrow_length_ratio=0.05,color='black')\n",
    "ax.quiver(-0.4,-0.4,-0.4,0,1,0,length=5,arrow_length_ratio=0.05,color='black')\n",
    "ax.quiver(-0.4,-0.4,-0.4,0,0,1,length=5,arrow_length_ratio=0.05,color='black')\n",
    "cbar=plt.colorbar(sm,ax=ax)\n",
    "cbar.set_label('Event count',rotation=270,fontsize=30,labelpad=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing inputs and predictions for Cf and IPNDV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=X_val.detach().numpy()[0]\n",
    "e=X_val.detach().numpy()[750]\n",
    "ef=np.zeros([4,4,4])\n",
    "ef[coords[:,0],coords[:,1],coords[:,2]]=e\n",
    "ef=np.rot90(ef,2,axes=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecounts=ef.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcounts=d+ecounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts=np.copy(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note this is trying to account for rotation of the detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts[:16]=dcounts[:16][::-1]\n",
    "dcounts[16:32]=dcounts[16:32][::-1]\n",
    "dcounts[32:48]=dcounts[32:48][::-1]\n",
    "dcounts[48:]=dcounts[48:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,2,figsize=(20,10))\n",
    "fig.set_tight_layout(True)\n",
    "ax[0,0].bar(np.arange(0,64,1),fcounts/fcounts.sum())\n",
    "ax[0,0].set_title('Cf cube counts,normalized',fontsize=22)\n",
    "ax[0,0].set_xlabel('Cube number',fontsize=18)\n",
    "ax[0,1].bar(np.arange(0,64,1),tcounts/tcounts.sum())\n",
    "ax[0,1].set_title('96 bare cube counts, normalized',fontsize=22)\n",
    "ax[0,1].set_xlabel('Cube number',fontsize=18)\n",
    "\n",
    "ax[1,1].bar(barcentres,model(torch.from_numpy(counts).to(torch.float)).detach().numpy(),edgecolor='xkcd:deep blue',width=np.diff(testbins))\n",
    "ax[1,1].set_xscale('log')\n",
    "ax[1,1].set_xlabel('Energy / MeV',fontsize=18)\n",
    "ax[1,1].set_ylabel('Frequency',fontsize=18)\n",
    "ax[1,1].set_title('Fluence prediction, 96 bare (IPNDV)',fontsize=22)\n",
    "\n",
    "ax[1,0].bar(barcentres,Y_val[750]+Y_val[0],edgecolor='xkcd:deep blue',width=np.diff(testbins))\n",
    "ax[1,0].set_xscale('log')\n",
    "ax[1,0].set_xlabel('Energy / MeV',fontsize=18)\n",
    "ax[1,0].set_title('Cf fluence target',fontsize=22)\n",
    "ax[1,0].set_ylabel('Frequency',fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond here is testing of dose calculation, unfinished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dose(binvals,centres,coeffs):\n",
    "    E=coeffs['Energy / MeV'].values\n",
    "    AP=coeffs['AP'].values\n",
    "    eff_dose=interp1d(E,AP)\n",
    "    dose=((binvals/400)*eff_dose(centres)).sum(axis=1)\n",
    "    return dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "E=coeffs['Energy / MeV'].values\n",
    "AP=coeffs['AP'].values\n",
    "eff_dose=interp1d(E,AP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "effserrs=np.load('/home/nr1315/Documents/Project/Simulations/Data/Low_scatter_old/simeffs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "effserrs[0]/=1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 7.7758e+02, 0.0000e+00, 1.5339e+03, 2.1336e+03, 1.5129e+03,\n",
       "        2.4050e+03, 2.8502e+03, 3.5835e+03, 4.0967e+03, 4.8158e+03, 5.3545e+03,\n",
       "        5.8893e+03, 6.2581e+03, 6.4400e+03, 5.8920e+03, 4.7713e+03, 3.3026e+03,\n",
       "        1.6761e+03, 5.9166e+02, 1.1313e+02, 0.0000e+00, 2.2123e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/nr1315/Documents/Project/NPL_2017-06-13/data/nemenix2_npl_2017-06-13_cf-4774_1m5_0deg_sc-stand_1V0_v12.h5',\n",
       " '/home/nr1315/Documents/Project/NPL_2017-06-13/data/nemenix2_npl_2017-06-14_ambe-30-1679_1m5_+0deg_sc-stand_1V0_v67.h5',\n",
       " '/home/nr1315/Documents/Project/NPL_2017-06-13/data/nemenix2_npl_2017-06-14_amli_1m5_+0deg_sc-stand_1V0_v55.h5',\n",
       " '/home/nr1315/Documents/Project/Simulations/Data/Low_scatter_new/4x4normal/thermal.h5']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dose_new(binvals,centres,coeffs,eff):\n",
    "    E=coeffs['Energy / MeV'].values\n",
    "    AP=coeffs['AP'].values\n",
    "    eff_dose=interp1d(E,AP)\n",
    "    efficiency=interp1d(eff[0],eff[1])\n",
    "    dose=((binvals.detach().numpy()/400))*eff_dose(centres)/efficiency(centres)\n",
    "    return dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "times=[]\n",
    "for file in xtrain[:3]:\n",
    "    df=pd.read_hdf(file)\n",
    "    time=df['time'].max()-df['time'].min()\n",
    "    times.append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency=interp1d(effserrs[0],effserrs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[95078.0, 6386.0, 30535.0]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_counts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sctime=64000*np.array(times)/np.array(X_counts[:3])/1e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.51202556,  0.        ,  0.71689412,  1.1773983 ,\n",
       "        1.04159186,  2.16550161,  3.47970837,  6.10912891,  9.55046197,\n",
       "       15.63753709, 24.05948994, 38.3344697 , 58.01321766, 75.54448203,\n",
       "       96.28895568, 89.83380318, 74.00285959, 46.41913814, 20.25659155,\n",
       "        4.87955757,  0.        ,  0.14702784,  0.        ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_dose_new(Y_val[0]/sctime[0],barcentres,coeffs,effserrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(barcentres,Y_val[0]/sctime[0],edgecolor='xkcd:deep blue',width=np.diff(testbins))\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 617.0620, 1067.6470, 1001.8536, 1323.8423,  544.2906, 1100.5436,\n",
       "        1080.6062,  866.2794,  460.5536,  665.9086,  654.9431,  593.1372,\n",
       "         257.1923,  328.9668,  439.6193,  380.8040, 1026.7753, 2423.3892,\n",
       "        2194.1091, 1863.1486,  830.3920, 1653.8060, 1790.3771, 1633.8687,\n",
       "         453.5755,  778.5549, 1151.3839,  821.4202,  455.5692,  435.6318,\n",
       "         462.5473,  471.5191, 1112.5060, 1838.2268, 2118.3472, 2001.7134,\n",
       "         939.0508, 1315.8673, 1655.7998, 1402.5950,  494.4471, 1030.7628,\n",
       "         836.3733,  833.3826,  518.3720,  474.5097,  550.2718,  452.5786,\n",
       "        1024.7815, 2227.0059, 2357.5957, 1797.3552,  783.5392,  917.1197,\n",
       "        1592.9969, 1201.2274,  556.2530,  632.0151,  913.1322,  847.3388,\n",
       "         382.7978,  434.6350,  481.4878,  446.5974])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 7.7758e+02, 0.0000e+00, 1.5339e+03, 2.1336e+03, 1.5129e+03,\n",
       "        2.4050e+03, 2.8502e+03, 3.5835e+03, 4.0967e+03, 4.8158e+03, 5.3545e+03,\n",
       "        5.8893e+03, 6.2581e+03, 6.4400e+03, 5.8920e+03, 4.7713e+03, 3.3026e+03,\n",
       "        1.6761e+03, 5.9166e+02, 1.1313e+02, 0.0000e+00, 2.2123e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
